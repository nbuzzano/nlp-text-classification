{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "import ast\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "import data_lake_helper as dl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake = dl_helper.DataLake(version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_to_prepare, key='text_normalized'):\n",
    "    train_x = df_to_prepare[key].tolist()\n",
    "    train_y = df_to_prepare.category.tolist()\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feature, load_version=None):\n",
    "\n",
    "    if load_version is None:\n",
    "        data_lake_ = data_lake\n",
    "    else:\n",
    "        data_lake_ = DataLake(version=load_version)\n",
    "    \n",
    "    df[feature] = data_lake_.load_obj(feature + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_lake.load_obj('df-cleaned.pkl')\n",
    "\n",
    "f_name = 'text_normalized'\n",
    "load_feature(f_name)\n",
    "\n",
    "load_feature('letter_lenght')\n",
    "\n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']\n",
    "\n",
    "#reducing df_train_table for testing purposes\n",
    "#n = 2500\n",
    "#df_train_table = df_train_table[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = prepare_data(df_train_table)\n",
    "valid_x, valid_y = prepare_data(df_test_table)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_types = sorted(df.category.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6646\n"
     ]
    }
   ],
   "source": [
    "# getting letter with highest words count\n",
    "#\n",
    "load_feature('letter_lenght')\n",
    "pad_sequence_max_len = round(max(df_train_table.letter_lenght.mean(), df_test_table.letter_lenght.mean()))#.max()\n",
    "pad_sequence_max_len = int(pad_sequence_max_len)\n",
    "del df['letter_lenght']\n",
    "print(pad_sequence_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df[f_name])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors\n",
    "train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_x), maxlen=pad_sequence_max_len)\n",
    "valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_x), maxlen=pad_sequence_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtengo la sentence tokenizada a partir del Tokenizer de keras\n",
    "word_index_ = {v:k for k,v in word_index.items()}\n",
    " \n",
    "def custom_tokenizer(text_to_tokenize):\n",
    "    text_sequences = tokenizer.texts_to_sequences([text_to_tokenize])[0]\n",
    "    return [word_index_[idx] for idx in text_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings\n",
    "##########################################\n",
    "\n",
    "# load the pre-trained word-embedding vectors\n",
    "\n",
    "try:\n",
    "    word2vec_model = data_lake.load_obj('word2vec_data' + '.pkl')\n",
    "\n",
    "except:\n",
    "    print(\"loading pre-trained word-embedding vectors...\")\n",
    "    time_start = time.time()\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        #creating Word2Vec item with list of list:\n",
    "        # - inner list represents a document\n",
    "        # - outer list represents the corpus\n",
    "        \n",
    "        text = df.iloc[i]['text_normalized']\n",
    "        data.append(custom_tokenizer(text))\n",
    "    \n",
    "    word2vec_model = Word2Vec(data)\n",
    "    data_lake.save_obj(word2vec_model,'word2vec_data' + '.pkl')\n",
    "    \n",
    "    print(word2vec_model)\n",
    "    print(\"loading finished - time: \", time.time() - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words 157052\n",
      "not_founded_words 103265\n",
      "embedding_matrix.shape (157053, 100)\n"
     ]
    }
   ],
   "source": [
    "# create token-embedding mapping\n",
    "\n",
    "# - If mask_zero is set to True, as a consequence, index 0 cannot be used\n",
    "#   in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "#\n",
    "\n",
    "not_founded_words = 0\n",
    "vector_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_dim), dtype='float32')\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    except:\n",
    "        not_founded_words += 1\n",
    "        pass\n",
    "\n",
    "print('total words', len(word_index))\n",
    "print('not_founded_words', not_founded_words)\n",
    "print('embedding_matrix.shape', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing feature that we wont't use anymore\n",
    "del df['text_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from numpy import save as numpy_save\n",
    "from numpy import load as numpy_load\n",
    "from keras import preprocessing\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    embedding_matrix_tsne = numpy_load('embedding_matrix_tsne.npy')\n",
    "except:\n",
    "    embedding_matrix_tsne = TSNE(n_components=2).fit_transform(embedding_matrix)\n",
    "    numpy_save('embedding_matrix_tsne.npy', embedding_matrix_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "\n",
    "# label encode the target variable \n",
    "train_y = df_train_table.category\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valores = [k for (k,v) in zip(df_train_table.category, train_y)]\n",
    "\n",
    "sns.scatterplot(embedding_matrix_tsne[:,0], embedding_matrix_tsne[:,1], hue=valores, legend='full', palette=palette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - If mask_zero is set to True, as a consequence, index 0 cannot be used\n",
    "#   in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "#\n",
    "\n",
    "def get_embedding_layer_v1(input_layer):\n",
    "    return layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                             output_dim=embedding_matrix.shape[1],\n",
    "                             weights=[embedding_matrix],\n",
    "                             mask_zero=True,\n",
    "                             trainable=False)(input_layer)\n",
    "    \n",
    "def get_embedding_layer_v2(input_layer):\n",
    "    \n",
    "    return layers.Embedding(input_dim=len(word_index) + 1, #size of vocab                  \n",
    "                             output_dim=vector_dim,\n",
    "                             mask_zero=True,\n",
    "                             trainable=True)(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_types = sorted(df.category.unique().tolist())\n",
    "output_layer_neurons = len(letter_types)\n",
    "\n",
    "def create_nn(layer_provider, embedding_version):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((pad_sequence_max_len, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    if embedding_version == 'v1':\n",
    "        embedding_layer = get_embedding_layer_v1(input_layer)\n",
    "    else:\n",
    "        embedding_layer = get_embedding_layer_v2(input_layer)\n",
    "        \n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add Custom Layer\n",
    "    custom_layer = layer_provider.build_layer(vector_dim, embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(custom_layer)\n",
    "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_layer_neurons, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy')\n",
    "    #sparse_categorical_crossentropy since we didnâ€™t one-hot encode the labels. \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn(classifier, nn_name):\n",
    "    try:\n",
    "        classifier = data_lake.load_obj(classifier, nn_name + '.pkl')\n",
    "    except:    \n",
    "        classifier.fit(train_seq_x, train_y,\n",
    "                       epochs=1, batch_size=128, validation_split = 0.2, verbose=1)\n",
    "        #data_lake.save_obj(classifier, nn_name +'.pkl')\n",
    "    \n",
    "    return classifier        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(classifier):\n",
    "    score = classifier.predict(valid_seq_x)\n",
    "\n",
    "    items_recall = recall_score(valid_y, score.argmax(axis=-1), average=None)\n",
    "\n",
    "    print('RECALL')\n",
    "    for item in zip(letter_types,items_recall):\n",
    "        print(str(item))\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(score.argmax(axis=-1), valid_y)\n",
    "    print('ACCURACY')\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'model_lstm'\n",
    "\n",
    "    def build_layer(self, vector_dim, prev_layer):\n",
    "        return layers.LSTM(vector_dim)(prev_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 183s 11s/step - loss: 1.8430 - val_loss: 1.2590\n",
      "RECALL\n",
      "('AK', 0.0)\n",
      "('AR', 0.025)\n",
      "('CL', 0.6109324758842444)\n",
      "('ER', 0.0)\n",
      "('FL', 0.0)\n",
      "('ND', 0.0)\n",
      "('RL', 0.9069767441860465)\n",
      "('TM', 0.0)\n",
      "('UR', 0.0)\n",
      "('UU', 0.0)\n",
      "ACCURACY\n",
      "0.5449620801733478\n"
     ]
    }
   ],
   "source": [
    "#train size = 2500\n",
    "lstm = create_nn(LSTMLayer(), embedding_version='v1')\n",
    "lstm = fit_nn(lstm, LSTMLayer().name)\n",
    "predict_nn(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 139s 9s/step - loss: 2.3240 - val_loss: 1.4822\n",
      "RECALL\n",
      "('AK', 0.0)\n",
      "('AR', 0.075)\n",
      "('CL', 0.7459807073954984)\n",
      "('ER', 0.0)\n",
      "('FL', 0.06930693069306931)\n",
      "('ND', 1.0)\n",
      "('RL', 0.005813953488372093)\n",
      "('TM', 0.0)\n",
      "('UR', 0.25)\n",
      "('UU', 0.01282051282051282)\n",
      "ACCURACY\n",
      "0.27085590465872156\n"
     ]
    }
   ],
   "source": [
    "class GRULayer:\n",
    "    def __init__(self):\n",
    "        self.name = 'model_gru'\n",
    "\n",
    "    def build_layer(self, vector_dim, prev_layer):\n",
    "        return layers.GRU(vector_dim)(prev_layer)\n",
    "\n",
    "#train size = 2500\n",
    "gru = create_nn(GRULayer(), embedding_version='v1')\n",
    "gru = fit_nn(gru, GRULayer().name)\n",
    "predict_nn(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bidirectional rnn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 406s 25s/step - loss: 1.7503 - val_loss: 1.0430\n",
      "RECALL\n",
      "('AK', 0.0)\n",
      "('AR', 0.05)\n",
      "('CL', 0.8102893890675241)\n",
      "('ER', 0.0)\n",
      "('FL', 0.0)\n",
      "('ND', 0.3333333333333333)\n",
      "('RL', 0.0)\n",
      "('TM', 0.0)\n",
      "('UR', 0.0)\n",
      "('UU', 0.01282051282051282)\n",
      "ACCURACY\n",
      "0.27735644637053086\n"
     ]
    }
   ],
   "source": [
    "class BidirectionalGRULayer:\n",
    "    def __init__(self):\n",
    "        self.name = 'model_bidirectional_gru'\n",
    "\n",
    "    def build_layer(self, vector_dim, prev_layer):\n",
    "        return layers.Bidirectional(layers.GRU(vector_dim))(prev_layer)\n",
    "\n",
    "#train size = 2500\n",
    "bidir_gru = create_nn(BidirectionalGRULayer(), embedding_version='v1')\n",
    "bidir_gru = fit_nn(bidir_gru, BidirectionalGRULayer().name)\n",
    "predict_nn(bidir_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 546s 34s/step - loss: 1.5833 - val_loss: 0.9639\n",
      "RECALL\n",
      "('AK', 0.0)\n",
      "('AR', 0.0)\n",
      "('CL', 0.8520900321543409)\n",
      "('ER', 0.0)\n",
      "('FL', 0.0)\n",
      "('ND', 0.0)\n",
      "('RL', 0.0029069767441860465)\n",
      "('TM', 0.0)\n",
      "('UR', 0.0)\n",
      "('UU', 0.0)\n",
      "ACCURACY\n",
      "0.28819068255687974\n"
     ]
    }
   ],
   "source": [
    "class BidirectionalLSTMLayer:\n",
    "    def __init__(self):\n",
    "        self.name = 'model_bidirectional_lstm'\n",
    "\n",
    "    def build_layer(self, vector_dim, prev_layer):\n",
    "        return layers.Bidirectional(layers.LSTM(vector_dim))(prev_layer)\n",
    "\n",
    "#train size = 2500\n",
    "bidir_lstm = create_nn(BidirectionalLSTMLayer(), embedding_version='v1')\n",
    "bidir_lstm = fit_nn(bidir_lstm, BidirectionalLSTMLayer().name)\n",
    "predict_nn(bidir_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final observations\n",
    "- We demonstrate that the LSTM, GRU and Bidirectional GRU networks are working properly. These were trained with the first 2500 items because the training task consumed many resources. Altough, the NNs training was not the best, because the first 2500 items were taken and therefore it is unknown if the validation dataset has a similar distribution to the training dataset, or even if it lacks some categories.\n",
    "\n",
    "- RandomSearch or GridSearch could be used to find the best classif hyperparameters, but since I'm focussing on learning different RNN's architectures I will pospose this.\n",
    "\n",
    "- Cross Validation is missing in ordern to estimate better how accurately the predictive model will perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
